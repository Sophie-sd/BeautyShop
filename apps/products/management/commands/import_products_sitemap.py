"""
–®–≤–∏–¥–∫–∏–π —ñ–º–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä—ñ–≤ —á–µ—Ä–µ–∑ sitemap.xml
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –æ–±–º–µ–∂–µ–Ω–æ—é –ø–∞–º'—è—Ç—Ç—é (512MB –Ω–∞ Render)
–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è: python manage.py import_products_sitemap
"""
import requests
from bs4 import BeautifulSoup
from django.core.management.base import BaseCommand
from django.core.files.base import ContentFile
from django.utils.text import slugify
from apps.products.models import Category, Product, ProductImage, ProductAttribute
from decimal import Decimal
import time
import re
import gc
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed


class Command(BaseCommand):
    help = '–®–≤–∏–¥–∫–∏–π —ñ–º–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä—ñ–≤ —á–µ—Ä–µ–∑ sitemap.xml'

    def __init__(self):
        super().__init__()
        self.base_url = 'https://beautyshop-ukrane.com.ua'
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.stats = {
            'products': 0,
            'images': 0,
            'errors': 0,
            'skipped': 0
        }
        self.default_category = None
        self.category_cache = {}
        
        self.URL_CATEGORY_MAP = {
            'nigti': 'nigti',
            'nogti': 'nigti',
            'nails': 'nigti',
            'gel-laki': 'nigti',
            'volosya': 'volossia',
            'volossya': 'volossia',
            'hair': 'volossia',
            'brovi': 'brovy-ta-vii',
            'brows': 'brovy-ta-vii',
            'vii': 'brovy-ta-vii',
            'depilyaciya': 'depilyatsiya',
            'depilacia': 'depilyatsiya',
            'shugaring': 'depilyatsiya',
            'vosk': 'depilyatsiya',
            'kosmetika': 'kosmetyka',
            'kosmetyka': 'kosmetyka',
            'makiyazh': 'makiyazh',
            'makeup': 'makiyazh',
            'odnorazova': 'odnorazova-produktsia',
            'disposable': 'odnorazova-produktsia',
            'dezinfekciya': 'dezinfektsiya-ta-sterylizatsiya',
            'sterilizaciya': 'dezinfektsiya-ta-sterylizatsiya',
            'mebli': 'mebli-dlya-saloniv',
            'furniture': 'mebli-dlya-saloniv',
        }

    def add_arguments(self, parser):
        parser.add_argument(
            '--limit',
            type=int,
            default=None,
            help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–≤–∞—Ä—ñ–≤'
        )
        parser.add_argument(
            '--skip-images',
            action='store_true',
            help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å'
        )
        parser.add_argument(
            '--workers',
            type=int,
            default=2,
            help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö –ø–æ—Ç–æ–∫—ñ–≤ (default: 2 –¥–ª—è –æ–±–º–µ–∂–µ–Ω–æ—ó –ø–∞–º\'—è—Ç—ñ)'
        )

    def handle(self, *args, **options):
        limit = options.get('limit')
        skip_images = options.get('skip_images', False)
        workers = min(options.get('workers', 2), 3)  # –ú–∞–∫—Å–∏–º—É–º 3 workers –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
        
        self.stdout.write(self.style.SUCCESS('üöÄ –Ü–º–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä—ñ–≤ —á–µ—Ä–µ–∑ Sitemap'))
        self.stdout.write('‚öôÔ∏è  –†–µ–∂–∏–º: –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è –æ–±–º–µ–∂–µ–Ω–æ—ó –ø–∞–º\'—è—Ç—ñ (512MB)\n')
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        self.default_category, _ = Category.objects.get_or_create(
            slug='import-webosova',
            defaults={'name': '–Ü–º–ø–æ—Ä—Ç –∑ Webosova', 'is_active': True}
        )
        
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö URL —Ç–æ–≤–∞—Ä—ñ–≤ –∑ sitemap
            product_urls = self.get_product_urls_from_sitemap()
            
            if not product_urls:
                self.stdout.write(self.style.ERROR('‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤ —É sitemap'))
                return
            
            if limit:
                product_urls = product_urls[:limit]
            
            total_products = len(product_urls)
            self.stdout.write(f'üì¶ –ó–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤: {total_products}')
            self.stdout.write(f'‚öôÔ∏è  –ü–æ—Ç–æ–∫—ñ–≤: {workers}\n')
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ —Ç–æ–≤–∞—Ä–∏ –±–∞—Ç—á–∞–º–∏ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
            batch_size = 50  # –ó–º–µ–Ω—à–µ–Ω–æ –¥–æ 50 –¥–ª—è Render
            for batch_start in range(0, total_products, batch_size):
                batch_end = min(batch_start + batch_size, total_products)
                batch_urls = product_urls[batch_start:batch_end]
                
                self.stdout.write(f'\nüì¶ –ë–∞—Ç—á {batch_start+1}-{batch_end}/{total_products}')
                
                # –Ü–º–ø–æ—Ä—Ç—É—î–º–æ —Ç–æ–≤–∞—Ä–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
                with ThreadPoolExecutor(max_workers=workers) as executor:
                    futures = {
                        executor.submit(self.import_product, url, skip_images): url 
                        for url in batch_urls
                    }
                    
                    for future in as_completed(futures):
                        url = futures[future]
                        try:
                            result = future.result(timeout=45)  # –ó–º–µ–Ω—à–∏–ª–∏ –¥–æ 45 —Å–µ–∫
                            if result:
                                self.stats['products'] += 1
                                if self.stats['products'] % 10 == 0:
                                    self.stdout.write(f'  ‚úì {self.stats["products"]}/{total_products}')
                        except TimeoutError:
                            self.stats['errors'] += 1
                        except Exception as e:
                            self.stats['errors'] += 1
                            # –ù–µ –≤–∏–≤–æ–¥–∏–º–æ –ø–æ–º–∏–ª–∫–∏ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
                
                # –ó–≤—ñ–ª—å–Ω—è—î–º–æ –ø–∞–º'—è—Ç—å –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ–≥–æ –±–∞—Ç—á—É
                gc.collect()
                time.sleep(2)  # –ó–±—ñ–ª—å—à–∏–ª–∏ –ø–∞—É–∑—É –¥–ª—è Render
            
            # –§—ñ–Ω–∞–ª—å–Ω–∞ –æ—á–∏—Å—Ç–∫–∞
            gc.collect()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            self.stdout.write(self.style.SUCCESS(f'\n‚úÖ –Ü–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ!'))
            self.stdout.write(f'üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:')
            self.stdout.write(f'  ‚Ä¢ –¢–æ–≤–∞—Ä—ñ–≤ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ: {self.stats["products"]}')
            self.stdout.write(f'  ‚Ä¢ –ó–æ–±—Ä–∞–∂–µ–Ω—å: {self.stats["images"]}')
            self.stdout.write(f'  ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ (–≤–∂–µ —ñ—Å–Ω—É—é—Ç—å): {self.stats["skipped"]}')
            if self.stats['errors'] > 0:
                self.stdout.write(self.style.WARNING(f'  ‚Ä¢ –ü–æ–º–∏–ª–æ–∫: {self.stats["errors"]}'))
                
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {str(e)}'))
            raise
        finally:
            # –§—ñ–Ω–∞–ª—å–Ω–∞ –æ—á–∏—Å—Ç–∫–∞
            gc.collect()

    def get_product_urls_from_sitemap(self):
        """–û—Ç—Ä–∏–º—É—î –≤—Å—ñ URL —Ç–æ–≤–∞—Ä—ñ–≤ –∑ sitemap.xml"""
        self.stdout.write('üó∫Ô∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è sitemap.xml...')
        
        try:
            response = self.session.get(f'{self.base_url}/sitemap.xml')
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'xml')
            urls = []
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ sitemap index
            sitemap_locs = soup.find_all('sitemap')
            if sitemap_locs:
                self.stdout.write(f'  –ó–Ω–∞–π–¥–µ–Ω–æ sitemap index –∑ {len(sitemap_locs)} —Ñ–∞–π–ª–∞–º–∏')
                # –¶–µ sitemap index - –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∫–æ–∂–µ–Ω –ø—ñ–¥—Ñ–∞–π–ª
                for sitemap in sitemap_locs:
                    loc = sitemap.find('loc')
                    if loc:
                        sitemap_url = loc.get_text()
                        self.stdout.write(f'  –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {sitemap_url}...')
                        urls.extend(self.parse_single_sitemap(sitemap_url))
            else:
                # –¶–µ –∑–≤–∏—á–∞–π–Ω–∏–π sitemap
                urls = self.parse_single_sitemap(f'{self.base_url}/sitemap.xml')
            
            return urls
            
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'–ü–æ–º–∏–ª–∫–∞: {str(e)}'))
            return []
    
    def parse_single_sitemap(self, sitemap_url):
        """–ü–∞—Ä—Å–∏—Ç—å –æ–∫—Ä–µ–º–∏–π sitemap —Ñ–∞–π–ª"""
        try:
            response = self.session.get(sitemap_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'xml')
            
            urls = []
            for loc in soup.find_all('loc'):
                url = loc.get_text()
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ä–æ—Å—ñ–π—Å—å–∫—É –≤–µ—Ä—Å—ñ—é —Å–∞–π—Ç—É
                if '/ru/' in url:
                    continue
                # –¢–æ–≤–∞—Ä–∏ –º–∞—é—Ç—å —Ñ–æ—Ä–º–∞—Ç /pXXXXXX
                if re.search(r'/p\d+', url):
                    urls.append(url)
            
            return urls
        except Exception as e:
            self.stdout.write(self.style.WARNING(f'    –ü–æ–º–∏–ª–∫–∞: {str(e)}'))
            return []

    def import_product(self, product_url, skip_images=False):
        """–Ü–º–ø–æ—Ä—Ç—É—î –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä"""
        try:
            response = self.session.get(product_url, timeout=25, stream=True)
            response.raise_for_status()
            
            # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
            content = b''
            for chunk in response.iter_content(chunk_size=8192):
                content += chunk
                if len(content) > 1024 * 1024:  # –ú–∞–∫—Å–∏–º—É–º 1MB
                    break
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # –ü–∞—Ä—Å–∏–º–æ –¥–∞–Ω—ñ
            data = self.parse_product_page(soup, product_url)
            
            if not data.get('name'):
                return None
            
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é –∑ URL
            category = self.detect_category_from_url(product_url)
            
            # –ì–µ–Ω–µ—Ä—É—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π slug
            base_slug = slugify(data['name'])
            slug = base_slug
            counter = 1
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ç–æ–≤–∞—Ä –≤–∂–µ —ñ—Å–Ω—É—î
            while Product.objects.filter(slug=slug).exists():
                # –Ø–∫—â–æ —ñ—Å–Ω—É—î —Ç–æ—á–Ω–∞ –Ω–∞–∑–≤–∞ - –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ (–≤–∂–µ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ)
                if counter == 1 and Product.objects.filter(name=data['name'][:200]).exists():
                    self.stats['skipped'] += 1
                    return None
                slug = f"{base_slug}-{counter}"
                counter += 1
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–æ–≤–∞—Ä
            product = Product.objects.create(
                name=data['name'][:200],
                slug=slug,
                category=category,
                description=data.get('description', '')[:2000],
                retail_price=data['price'],
                is_active=True,
                stock=100,
            )
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            if not skip_images and data.get('images'):
                self.download_images(product, data['images'])
            
            # –î–æ–¥–∞—î–º–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            if data.get('attributes'):
                self.create_attributes(product, data['attributes'])
            
            return product
            
        except Exception as e:
            raise Exception(f'–ü–æ–º–∏–ª–∫–∞ —ñ–º–ø–æ—Ä—Ç—É: {str(e)}')

    def parse_product_page(self, soup, url):
        """–ü–∞—Ä—Å–∏—Ç—å –¥–∞–Ω—ñ —Ç–æ–≤–∞—Ä—É"""
        data = {'images': [], 'attributes': []}
        
        # –ù–∞–∑–≤–∞
        h1 = soup.find('h1')
        if h1:
            data['name'] = h1.get_text(strip=True)
        
        # –¶—ñ–Ω–∞ - —à—É–∫–∞—î–º–æ –≤ id="product"
        product_block = soup.find(id='product')
        if not product_block:
            product_block = soup.find('div', class_=re.compile('product-info'))
        
        if product_block:
            price_elem = product_block.find(class_=re.compile('price'))
            if price_elem:
                price_text = price_elem.get_text()
                price_match = re.search(r'(\d+(?:\.\d+)?)\s*–≥—Ä–Ω', price_text)
                if price_match:
                    data['price'] = Decimal(price_match.group(1))
        
        if 'price' not in data:
            data['price'] = Decimal('100.00')  # –î–µ—Ñ–æ–ª—Ç–Ω–∞ —Ü—ñ–Ω–∞
        
        # –û–ø–∏—Å
        desc_blocks = soup.find_all(['div', 'section'], class_=re.compile('description|tab-description'))
        for desc in desc_blocks:
            text = desc.get_text(strip=True)
            if len(text) > 50:
                data['description'] = text[:2000]
                break
        
        # –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src and 'catalog' in src and 'product' in src:
                if 'icon' not in src and 'logo' not in src:
                    full_url = urljoin(url, src)
                    full_url = full_url.replace('/cachewebp/', '/cache/').replace('.webp', '.jpg')
                    if full_url not in data['images']:
                        data['images'].append(full_url)
        
        return data

    def download_images(self, product, image_urls):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (–æ–±–º–µ–∂–µ–Ω–æ 2 –Ω–∞ —Ç–æ–≤–∞—Ä –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ)"""
        for idx, img_url in enumerate(image_urls[:2]):  # –¢—ñ–ª—å–∫–∏ 2 –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞–º—ñ—Å—Ç—å 3
            try:
                response = self.session.get(img_url, timeout=15)  # –ó–º–µ–Ω—à–∏–ª–∏ —Ç–∞–π–º–∞—É—Ç
                response.raise_for_status()
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É (–ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —è–∫—â–æ > 2MB)
                if len(response.content) > 2 * 1024 * 1024:
                    continue
                
                ext = 'jpg'
                if 'content-type' in response.headers:
                    if 'png' in response.headers['content-type']:
                        ext = 'png'
                
                filename = f"{product.slug}_{idx+1}.{ext}"
                product_image = ProductImage(
                    product=product,
                    is_main=(idx == 0),
                    sort_order=idx
                )
                product_image.image.save(filename, ContentFile(response.content), save=True)
                self.stats['images'] += 1
                
                # –û—á–∏—â—É—î–º–æ response –∑ –ø–∞–º'—è—Ç—ñ
                del response
                
            except Exception as e:
                pass

    def detect_category_from_url(self, url):
        """–í–∏–∑–Ω–∞—á–∞—î –∫–∞—Ç–µ–≥–æ—Ä—ñ—é –∑ URL —Ç–æ–≤–∞—Ä—É"""
        url_lower = url.lower()
        
        for url_part, category_slug in self.URL_CATEGORY_MAP.items():
            if f'/{url_part}/' in url_lower or f'/{url_part}-' in url_lower:
                if category_slug not in self.category_cache:
                    cat = Category.objects.filter(slug=category_slug).first()
                    self.category_cache[category_slug] = cat if cat else self.default_category
                return self.category_cache[category_slug]
        
        return self.default_category
    
    def create_attributes(self, product, attributes):
        """–°—Ç–≤–æ—Ä—é—î —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"""
        for idx, attr in enumerate(attributes[:20]):
            ProductAttribute.objects.create(
                product=product,
                name=attr['name'][:100],
                value=attr['value'][:200],
                sort_order=idx
            )

