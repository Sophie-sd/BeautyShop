"""
–®–≤–∏–¥–∫–∏–π —ñ–º–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä—ñ–≤ —á–µ—Ä–µ–∑ sitemap.xml
–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è: python manage.py import_products_sitemap
"""
import requests
from bs4 import BeautifulSoup
from django.core.management.base import BaseCommand
from django.core.files.base import ContentFile
from django.utils.text import slugify
from apps.products.models import Category, Product, ProductImage, ProductAttribute
from decimal import Decimal
import time
import re
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed


class Command(BaseCommand):
    help = '–®–≤–∏–¥–∫–∏–π —ñ–º–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä—ñ–≤ —á–µ—Ä–µ–∑ sitemap.xml'

    def __init__(self):
        super().__init__()
        self.base_url = 'https://beautyshop-ukrane.com.ua'
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.stats = {
            'products': 0,
            'images': 0,
            'errors': 0,
            'skipped': 0
        }
        self.default_category = None

    def add_arguments(self, parser):
        parser.add_argument(
            '--limit',
            type=int,
            default=None,
            help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–≤–∞—Ä—ñ–≤'
        )
        parser.add_argument(
            '--skip-images',
            action='store_true',
            help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å'
        )
        parser.add_argument(
            '--workers',
            type=int,
            default=5,
            help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö –ø–æ—Ç–æ–∫—ñ–≤ (default: 5)'
        )

    def handle(self, *args, **options):
        limit = options.get('limit')
        skip_images = options.get('skip_images', False)
        workers = options.get('workers', 5)
        
        self.stdout.write(self.style.SUCCESS('üöÄ –Ü–º–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä—ñ–≤ —á–µ—Ä–µ–∑ Sitemap'))
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        self.default_category, _ = Category.objects.get_or_create(
            slug='import-webosova',
            defaults={'name': '–Ü–º–ø–æ—Ä—Ç –∑ Webosova', 'is_active': True}
        )
        
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö URL —Ç–æ–≤–∞—Ä—ñ–≤ –∑ sitemap
            product_urls = self.get_product_urls_from_sitemap()
            
            if not product_urls:
                self.stdout.write(self.style.ERROR('‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤ —É sitemap'))
                return
            
            if limit:
                product_urls = product_urls[:limit]
            
            self.stdout.write(f'üì¶ –ó–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤: {len(product_urls)}')
            self.stdout.write(f'‚öôÔ∏è  –ü–æ—Ç–æ–∫—ñ–≤: {workers}\n')
            
            # –Ü–º–ø–æ—Ä—Ç—É—î–º–æ —Ç–æ–≤–∞—Ä–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
            with ThreadPoolExecutor(max_workers=workers) as executor:
                futures = {
                    executor.submit(self.import_product, url, skip_images): url 
                    for url in product_urls
                }
                
                for future in as_completed(futures):
                    url = futures[future]
                    try:
                        result = future.result()
                        if result:
                            self.stats['products'] += 1
                            if self.stats['products'] % 50 == 0:
                                self.stdout.write(f'  ‚úì –Ü–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ: {self.stats["products"]}')
                    except Exception as e:
                        self.stats['errors'] += 1
                        self.stdout.write(self.style.WARNING(f'  ‚ö† –ü–æ–º–∏–ª–∫–∞ {url}: {str(e)[:50]}'))
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            self.stdout.write(self.style.SUCCESS(f'\n‚úÖ –Ü–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ!'))
            self.stdout.write(f'üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:')
            self.stdout.write(f'  ‚Ä¢ –¢–æ–≤–∞—Ä—ñ–≤ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ: {self.stats["products"]}')
            self.stdout.write(f'  ‚Ä¢ –ó–æ–±—Ä–∞–∂–µ–Ω—å: {self.stats["images"]}')
            self.stdout.write(f'  ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ (–≤–∂–µ —ñ—Å–Ω—É—é—Ç—å): {self.stats["skipped"]}')
            if self.stats['errors'] > 0:
                self.stdout.write(self.style.WARNING(f'  ‚Ä¢ –ü–æ–º–∏–ª–æ–∫: {self.stats["errors"]}'))
                
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {str(e)}'))
            raise

    def get_product_urls_from_sitemap(self):
        """–û—Ç—Ä–∏–º—É—î –≤—Å—ñ URL —Ç–æ–≤–∞—Ä—ñ–≤ –∑ sitemap.xml"""
        self.stdout.write('üó∫Ô∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è sitemap.xml...')
        
        try:
            response = self.session.get(f'{self.base_url}/sitemap.xml')
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'xml')
            urls = []
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ sitemap index
            sitemap_locs = soup.find_all('sitemap')
            if sitemap_locs:
                self.stdout.write(f'  –ó–Ω–∞–π–¥–µ–Ω–æ sitemap index –∑ {len(sitemap_locs)} —Ñ–∞–π–ª–∞–º–∏')
                # –¶–µ sitemap index - –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∫–æ–∂–µ–Ω –ø—ñ–¥—Ñ–∞–π–ª
                for sitemap in sitemap_locs:
                    loc = sitemap.find('loc')
                    if loc:
                        sitemap_url = loc.get_text()
                        self.stdout.write(f'  –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {sitemap_url}...')
                        urls.extend(self.parse_single_sitemap(sitemap_url))
            else:
                # –¶–µ –∑–≤–∏—á–∞–π–Ω–∏–π sitemap
                urls = self.parse_single_sitemap(f'{self.base_url}/sitemap.xml')
            
            return urls
            
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'–ü–æ–º–∏–ª–∫–∞: {str(e)}'))
            return []
    
    def parse_single_sitemap(self, sitemap_url):
        """–ü–∞—Ä—Å–∏—Ç—å –æ–∫—Ä–µ–º–∏–π sitemap —Ñ–∞–π–ª"""
        try:
            response = self.session.get(sitemap_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'xml')
            
            urls = []
            for loc in soup.find_all('loc'):
                url = loc.get_text()
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ä–æ—Å—ñ–π—Å—å–∫—É –≤–µ—Ä—Å—ñ—é —Å–∞–π—Ç—É
                if '/ru/' in url:
                    continue
                # –¢–æ–≤–∞—Ä–∏ –º–∞—é—Ç—å —Ñ–æ—Ä–º–∞—Ç /pXXXXXX
                if re.search(r'/p\d+', url):
                    urls.append(url)
            
            return urls
        except Exception as e:
            self.stdout.write(self.style.WARNING(f'    –ü–æ–º–∏–ª–∫–∞: {str(e)}'))
            return []

    def import_product(self, product_url, skip_images=False):
        """–Ü–º–ø–æ—Ä—Ç—É—î –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä"""
        try:
            response = self.session.get(product_url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ü–∞—Ä—Å–∏–º–æ –¥–∞–Ω—ñ
            data = self.parse_product_page(soup, product_url)
            
            if not data.get('name'):
                return None
            
            # –ì–µ–Ω–µ—Ä—É—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π slug
            base_slug = slugify(data['name'])
            slug = base_slug
            counter = 1
            while Product.objects.filter(slug=slug).exists():
                slug = f"{base_slug}-{counter}"
                counter += 1
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–æ–≤–∞—Ä
            product = Product.objects.create(
                name=data['name'][:200],  # –û–±–º–µ–∂—É—î–º–æ –¥–æ–≤–∂–∏–Ω—É
                slug=slug,
                category=self.default_category,
                description=data.get('description', '')[:2000],
                retail_price=data['price'],
                is_active=True,
                stock=100,
            )
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            if not skip_images and data.get('images'):
                self.download_images(product, data['images'])
            
            # –î–æ–¥–∞—î–º–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            if data.get('attributes'):
                self.create_attributes(product, data['attributes'])
            
            return product
            
        except Exception as e:
            raise Exception(f'–ü–æ–º–∏–ª–∫–∞ —ñ–º–ø–æ—Ä—Ç—É: {str(e)}')

    def parse_product_page(self, soup, url):
        """–ü–∞—Ä—Å–∏—Ç—å –¥–∞–Ω—ñ —Ç–æ–≤–∞—Ä—É"""
        data = {'images': [], 'attributes': []}
        
        # –ù–∞–∑–≤–∞
        h1 = soup.find('h1')
        if h1:
            data['name'] = h1.get_text(strip=True)
        
        # –¶—ñ–Ω–∞ - —à—É–∫–∞—î–º–æ –≤ id="product"
        product_block = soup.find(id='product')
        if not product_block:
            product_block = soup.find('div', class_=re.compile('product-info'))
        
        if product_block:
            price_elem = product_block.find(class_=re.compile('price'))
            if price_elem:
                price_text = price_elem.get_text()
                price_match = re.search(r'(\d+(?:\.\d+)?)\s*–≥—Ä–Ω', price_text)
                if price_match:
                    data['price'] = Decimal(price_match.group(1))
        
        if 'price' not in data:
            data['price'] = Decimal('100.00')  # –î–µ—Ñ–æ–ª—Ç–Ω–∞ —Ü—ñ–Ω–∞
        
        # –û–ø–∏—Å
        desc_blocks = soup.find_all(['div', 'section'], class_=re.compile('description|tab-description'))
        for desc in desc_blocks:
            text = desc.get_text(strip=True)
            if len(text) > 50:
                data['description'] = text[:2000]
                break
        
        # –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src and 'catalog' in src and 'product' in src:
                if 'icon' not in src and 'logo' not in src:
                    full_url = urljoin(url, src)
                    full_url = full_url.replace('/cachewebp/', '/cache/').replace('.webp', '.jpg')
                    if full_url not in data['images']:
                        data['images'].append(full_url)
        
        return data

    def download_images(self, product, image_urls):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è"""
        for idx, img_url in enumerate(image_urls[:3]):
            try:
                response = self.session.get(img_url, timeout=10)
                response.raise_for_status()
                
                ext = 'jpg'
                if 'content-type' in response.headers:
                    if 'png' in response.headers['content-type']:
                        ext = 'png'
                
                filename = f"{product.slug}_{idx+1}.{ext}"
                product_image = ProductImage(
                    product=product,
                    is_main=(idx == 0),
                    sort_order=idx
                )
                product_image.image.save(filename, ContentFile(response.content), save=True)
                self.stats['images'] += 1
                
            except Exception as e:
                pass

    def create_attributes(self, product, attributes):
        """–°—Ç–≤–æ—Ä—é—î —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"""
        for idx, attr in enumerate(attributes[:20]):
            ProductAttribute.objects.create(
                product=product,
                name=attr['name'][:100],
                value=attr['value'][:200],
                sort_order=idx
            )

