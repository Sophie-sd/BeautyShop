"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π —ñ–º–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è Render –∑ Cloudinary
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –ø—ñ–¥ —á–∞—Å build –ø—Ä–æ—Ü–µ—Å—É
–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—é—Ç—å—Å—è –Ω–∞ Cloudinary —á–µ—Ä–µ–∑ Django STORAGES
"""
import requests
from bs4 import BeautifulSoup
from django.core.management.base import BaseCommand
from django.core.files.base import ContentFile
from django.utils.text import slugify
from apps.products.models import Category, Product, ProductImage, ProductAttribute
from decimal import Decimal
import time
import re
import gc
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed


class Command(BaseCommand):
    help = '–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π —ñ–º–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä—ñ–≤ –∑ Cloudinary –¥–ª—è Render'

    def __init__(self):
        super().__init__()
        self.base_url = 'https://webosova.com.ua'
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.stats = {
            'products': 0,
            'images': 0,
            'errors': 0,
            'skipped': 0
        }
        self.default_category = None
        self.category_cache = {}
        
        # –ú–∞–ø—ñ–Ω–≥ URL –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –Ω–∞ –Ω–∞—à—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
        self.URL_CATEGORY_MAP = {
            'nigti': 'nigti',
            'nogti': 'nigti',
            'nails': 'nigti',
            'gel-laki': 'nigti',
            'gel-lak': 'nigti',
            'baza': 'nigti',
            'top': 'nigti',
            'volosya': 'volossia',
            'volossya': 'volossia',
            'volossia': 'volossia',
            'hair': 'volossia',
            'shampun': 'volossia',
            'maska': 'volossia',
            'brovi': 'brovy-ta-vii',
            'brows': 'brovy-ta-vii',
            'vii': 'brovy-ta-vii',
            'resnicy': 'brovy-ta-vii',
            'depilyaciya': 'depilyatsiya',
            'depilacia': 'depilyatsiya',
            'depilyatsiya': 'depilyatsiya',
            'shugaring': 'depilyatsiya',
            'vosk': 'depilyatsiya',
            'pasta': 'depilyatsiya',
            'kosmetika': 'kosmetyka',
            'kosmetyka': 'kosmetyka',
            'krem': 'kosmetyka',
            'serum': 'kosmetyka',
            'makiyazh': 'makiyazh',
            'makeup': 'makiyazh',
            'pomada': 'makiyazh',
            'tush': 'makiyazh',
            'odnorazova': 'odnorazova-produktsia',
            'disposable': 'odnorazova-produktsia',
            'rukavychky': 'odnorazova-produktsia',
            'masochky': 'odnorazova-produktsia',
            'dezinfekciya': 'dezinfektsiya-ta-sterylizatsiya',
            'sterilizaciya': 'dezinfektsiya-ta-sterylizatsiya',
            'dezinfektsiya': 'dezinfektsiya-ta-sterylizatsiya',
            'antiseptyk': 'dezinfektsiya-ta-sterylizatsiya',
            'mebli': 'mebli-dlya-saloniv',
            'furniture': 'mebli-dlya-saloniv',
            'kreslo': 'mebli-dlya-saloniv',
            'kushetka': 'mebli-dlya-saloniv',
            'kosmetologiya': 'kosmetyka',
            'tara': 'odnorazova-produktsia',
            'sale': 'sale',
            'akciya': 'sale',
            'znuzhky': 'sale',
        }

    def add_arguments(self, parser):
        parser.add_argument(
            '--batch-size',
            type=int,
            default=200,
            help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–≤–∞—Ä—ñ–≤ –≤ –æ–¥–Ω–æ–º—É –±–∞—Ç—á—ñ'
        )
        parser.add_argument(
            '--workers',
            type=int,
            default=2,
            help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö –ø–æ—Ç–æ–∫—ñ–≤'
        )
        parser.add_argument(
            '--limit',
            type=int,
            default=None,
            help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É'
        )

    def handle(self, *args, **options):
        batch_size = options.get('batch_size', 200)
        workers = min(options.get('workers', 2), 3)
        limit = options.get('limit')
        
        self.stdout.write(self.style.SUCCESS('üöÄ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π —ñ–º–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è Render'))
        self.stdout.write('üì° –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞–≤–∞–Ω—Ç–∞–∂—É—é—Ç—å—Å—è –Ω–∞ Cloudinary\n')
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        self.default_category, _ = Category.objects.get_or_create(
            slug='import-webosova',
            defaults={'name': '–Ü–º–ø–æ—Ä—Ç –∑ Webosova', 'is_active': True}
        )
        
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ URL —Ç–æ–≤–∞—Ä—ñ–≤ –∑ sitemap
            product_urls = self.get_product_urls_from_sitemap()
            
            if not product_urls:
                self.stdout.write(self.style.ERROR('‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤ —É sitemap'))
                return
            
            if limit:
                product_urls = product_urls[:limit]
            
            total_products = len(product_urls)
            self.stdout.write(f'üì¶ –ó–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤: {total_products}')
            self.stdout.write(f'‚öôÔ∏è  –ë–∞—Ç—á: {batch_size}, –ü–æ—Ç–æ–∫—ñ–≤: {workers}\n')
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ –ø–æ –±–∞—Ç—á–∞–º
            for batch_start in range(0, total_products, batch_size):
                batch_end = min(batch_start + batch_size, total_products)
                batch_urls = product_urls[batch_start:batch_end]
                
                self.stdout.write(f'\nüì¶ –ë–∞—Ç—á {batch_start+1}-{batch_end}/{total_products}')
                
                # –Ü–º–ø–æ—Ä—Ç—É—î–º–æ —Ç–æ–≤–∞—Ä–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
                with ThreadPoolExecutor(max_workers=workers) as executor:
                    futures = {
                        executor.submit(self.import_product, url): url 
                        for url in batch_urls
                    }
                    
                    for future in as_completed(futures):
                        url = futures[future]
                        try:
                            result = future.result(timeout=60)
                            if result:
                                self.stats['products'] += 1
                                if self.stats['products'] % 25 == 0:
                                    self.stdout.write(
                                        f'  ‚úì {self.stats["products"]}/{total_products} '
                                        f'(üì∑ {self.stats["images"]} –∑–æ–±—Ä.)'
                                    )
                        except TimeoutError:
                            self.stats['errors'] += 1
                        except Exception:
                            self.stats['errors'] += 1
                
                # –û—á–∏—â—É—î–º–æ –ø–∞–º'—è—Ç—å
                gc.collect()
                time.sleep(2)
            
            # –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            gc.collect()
            
            self.stdout.write(self.style.SUCCESS(f'\n‚úÖ –Ü–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ!'))
            self.stdout.write(f'üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:')
            self.stdout.write(f'  ‚Ä¢ –¢–æ–≤–∞—Ä—ñ–≤ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ: {self.stats["products"]}')
            self.stdout.write(f'  ‚Ä¢ –ó–æ–±—Ä–∞–∂–µ–Ω—å –Ω–∞ Cloudinary: {self.stats["images"]}')
            self.stdout.write(f'  ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª—ñ–∫–∞—Ç–∏): {self.stats["skipped"]}')
            if self.stats['errors'] > 0:
                self.stdout.write(self.style.WARNING(f'  ‚Ä¢ –ü–æ–º–∏–ª–æ–∫: {self.stats["errors"]}'))
            
            self.stdout.write('\nüìÇ –¢–æ–≤–∞—Ä–∏ –¥–æ–¥–∞–Ω—ñ –≤ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–∞ –≥–æ—Ç–æ–≤—ñ –¥–æ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó!')
                
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {str(e)}'))
            raise
        finally:
            gc.collect()

    def get_product_urls_from_sitemap(self):
        """–û—Ç—Ä–∏–º—É—î –≤—Å—ñ URL —Ç–æ–≤–∞—Ä—ñ–≤ –∑ sitemap.xml"""
        self.stdout.write('üó∫Ô∏è  –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è sitemap.xml...')
        
        try:
            response = self.session.get(f'{self.base_url}/sitemap.xml', timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'xml')
            urls = []
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ sitemap index
            sitemap_locs = soup.find_all('sitemap')
            if sitemap_locs:
                for sitemap in sitemap_locs:
                    loc = sitemap.find('loc')
                    if loc:
                        urls.extend(self.parse_single_sitemap(loc.get_text()))
            else:
                urls = self.parse_single_sitemap(f'{self.base_url}/sitemap.xml')
            
            return urls
            
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'–ü–æ–º–∏–ª–∫–∞: {str(e)}'))
            return []
    
    def parse_single_sitemap(self, sitemap_url):
        """–ü–∞—Ä—Å–∏—Ç—å –æ–∫—Ä–µ–º–∏–π sitemap —Ñ–∞–π–ª"""
        try:
            response = self.session.get(sitemap_url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'xml')
            
            urls = []
            for loc in soup.find_all('loc'):
                url = loc.get_text()
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ä–æ—Å—ñ–π—Å—å–∫—É –≤–µ—Ä—Å—ñ—é
                if '/ru/' in url:
                    continue
                # –¢–æ–≤–∞—Ä–∏ –º–∞—é—Ç—å —Ñ–æ—Ä–º–∞—Ç /pXXXXXX
                if re.search(r'/p\d+', url):
                    urls.append(url)
            
            return urls
        except Exception:
            return []

    def import_product(self, product_url):
        """–Ü–º–ø–æ—Ä—Ç—É—î –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä –∑ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è–º–∏ –Ω–∞ Cloudinary"""
        try:
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É —Ç–æ–≤–∞—Ä—É –∑ retry
            for attempt in range(2):
                try:
                    response = self.session.get(product_url, timeout=30, stream=True)
                    response.raise_for_status()
                    break
                except Exception:
                    if attempt == 0:
                        time.sleep(1)
                        continue
                    raise
            
            # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó
            content = b''
            for chunk in response.iter_content(chunk_size=8192):
                content += chunk
                if len(content) > 1024 * 1024:
                    break
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # –ü–∞—Ä—Å–∏–º–æ –¥–∞–Ω—ñ —Ç–æ–≤–∞—Ä—É
            data = self.parse_product_page(soup, product_url)
            
            if not data.get('name'):
                return None
            
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é
            category = self.detect_category_from_url(product_url)
            
            # –ì–µ–Ω–µ—Ä—É—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π slug
            base_slug = slugify(data['name'])
            slug = base_slug
            counter = 1
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏
            while Product.objects.filter(slug=slug).exists():
                if counter == 1 and Product.objects.filter(name=data['name'][:200]).exists():
                    self.stats['skipped'] += 1
                    return None
                slug = f"{base_slug}-{counter}"
                counter += 1
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–æ–≤–∞—Ä –∑ —É—Å—ñ–º–∞ –ø–æ–ª—è–º–∏
            product = Product.objects.create(
                name=data['name'][:200],
                slug=slug,
                category=category,
                description=data.get('description', '')[:2000],
                retail_price=data['price'],
                is_active=True,
                stock=100,
                # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–æ–ª—è –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
                is_new=False,
                is_top=False,
                is_sale=False,
                is_featured=False,
            )
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –Ω–∞ Cloudinary –≤ production)
            if data.get('images'):
                self.download_images_to_cloudinary(product, data['images'])
            
            # –î–æ–¥–∞—î–º–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            if data.get('attributes'):
                self.create_attributes(product, data['attributes'])
            
            return product
            
        except Exception as e:
            raise Exception(f'–ü–æ–º–∏–ª–∫–∞ —ñ–º–ø–æ—Ä—Ç—É {product_url}: {str(e)}')

    def parse_product_page(self, soup, url):
        """–ü–∞—Ä—Å–∏—Ç—å –¥–∞–Ω—ñ –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Ç–æ–≤–∞—Ä—É"""
        data = {'images': [], 'attributes': []}
        
        # –ù–∞–∑–≤–∞
        h1 = soup.find('h1')
        if h1:
            data['name'] = h1.get_text(strip=True)
        
        # –¶—ñ–Ω–∞
        product_block = soup.find(id='product') or soup.find('div', class_=re.compile('product-info'))
        if product_block:
            price_elem = product_block.find(class_=re.compile('price'))
            if price_elem:
                price_text = price_elem.get_text()
                price_match = re.search(r'(\d+(?:\.\d+)?)\s*–≥—Ä–Ω', price_text)
                if price_match:
                    data['price'] = Decimal(price_match.group(1))
        
        if 'price' not in data:
            data['price'] = Decimal('100.00')
        
        # –û–ø–∏—Å
        desc_blocks = soup.find_all(['div', 'section'], class_=re.compile('description|tab-description|product-desc'))
        for desc in desc_blocks:
            text = desc.get_text(strip=True)
            if len(text) > 50:
                data['description'] = text[:2000]
                break
        
        # –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è - –∑–±–∏—Ä–∞—î–º–æ –≤—Å—ñ –º–æ–∂–ª–∏–≤—ñ
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-lazy')
            if src and ('catalog' in src or 'product' in src):
                if 'icon' not in src and 'logo' not in src and 'banner' not in src:
                    full_url = urljoin(url, src)
                    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ webp –Ω–∞ jpg
                    full_url = full_url.replace('/cachewebp/', '/cache/').replace('.webp', '.jpg')
                    if full_url not in data['images']:
                        data['images'].append(full_url)
        
        # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ - –ø–∞—Ä—Å–∏–º–æ –∑ —Ç–∞–±–ª–∏—Ü—å
        tables = soup.find_all('table', class_=re.compile('attr|char|spec|properties'))
        for table in tables:
            for row in table.find_all('tr'):
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    name = cells[0].get_text(strip=True)
                    value = cells[1].get_text(strip=True)
                    if name and value:
                        data['attributes'].append({'name': name, 'value': value})
        
        return data

    def download_images_to_cloudinary(self, product, image_urls):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–∞ Cloudinary"""
        for idx, img_url in enumerate(image_urls[:3]):
            try:
                response = self.session.get(img_url, timeout=20)
                response.raise_for_status()
                
                if len(response.content) > 5 * 1024 * 1024:
                    continue
                
                ext = 'jpg'
                if 'content-type' in response.headers and 'png' in response.headers['content-type']:
                    ext = 'png'
                
                filename = f"{product.slug}_{idx+1}.{ext}"
                
                product_image = ProductImage(
                    product=product,
                    is_main=(idx == 0),
                    sort_order=idx,
                    alt_text=product.name
                )
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ –Ω–∞ Cloudinary –±–µ–∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
                product_image.image.save(filename, ContentFile(response.content), save=True)
                
                self.stats['images'] += 1
                del response
                
            except Exception as e:
                # –î–µ—Ç–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫
                import sys
                print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è {img_url[:80]}: {str(e)}", file=sys.stderr)
                pass

    def detect_category_from_url(self, url):
        """–í–∏–∑–Ω–∞—á–∞—î –∫–∞—Ç–µ–≥–æ—Ä—ñ—é –Ω–∞ –æ—Å–Ω–æ–≤—ñ URL —Ç–æ–≤–∞—Ä—É"""
        url_lower = url.lower()
        
        for url_part, category_slug in self.URL_CATEGORY_MAP.items():
            if f'/{url_part}/' in url_lower or f'/{url_part}-' in url_lower or f'-{url_part}/' in url_lower:
                if category_slug not in self.category_cache:
                    cat = Category.objects.filter(slug=category_slug).first()
                    self.category_cache[category_slug] = cat if cat else self.default_category
                return self.category_cache[category_slug]
        
        return self.default_category
    
    def create_attributes(self, product, attributes):
        """–°—Ç–≤–æ—Ä—é—î —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–≤–∞—Ä—É –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó"""
        for idx, attr in enumerate(attributes[:30]):  # –î–æ 30 —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            try:
                ProductAttribute.objects.create(
                    product=product,
                    name=attr['name'][:100],
                    value=attr['value'][:200],
                    sort_order=idx
                )
            except Exception:
                pass  # –Ü–≥–Ω–æ—Ä—É—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏

